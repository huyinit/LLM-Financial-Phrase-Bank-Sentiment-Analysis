{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3f54a6",
   "metadata": {
    "papermill": {
     "duration": 0.010876,
     "end_time": "2024-06-20T21:25:16.435773",
     "exception": false,
     "start_time": "2024-06-20T21:25:16.424897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fine-tune Llama 3 for Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e5646",
   "metadata": {
    "papermill": {
     "duration": 0.011015,
     "end_time": "2024-06-20T21:25:16.497909",
     "exception": false,
     "start_time": "2024-06-20T21:25:16.486894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e4d9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:25:16.520382Z",
     "iopub.status.busy": "2024-06-20T21:25:16.519824Z",
     "iopub.status.idle": "2024-06-20T21:27:29.808274Z",
     "shell.execute_reply": "2024-06-20T21:27:29.807291Z"
    },
    "papermill": {
     "duration": 133.302294,
     "end_time": "2024-06-20T21:27:29.810711",
     "exception": false,
     "start_time": "2024-06-20T21:25:16.508417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "# !pip install -q -U transformers==\"4.40.0\"\n",
    "# !pip install -q -U accelerate\n",
    "# !pip install -q -U datasets\n",
    "# !pip install -q -U trl\n",
    "# !pip install -q -U peft\n",
    "# !pip install -q -U tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be251ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d861896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:29.855163Z",
     "iopub.status.busy": "2024-06-20T21:27:29.854854Z",
     "iopub.status.idle": "2024-06-20T21:27:29.859361Z",
     "shell.execute_reply": "2024-06-20T21:27:29.858514Z"
    },
    "papermill": {
     "duration": 0.018282,
     "end_time": "2024-06-20T21:27:29.861276",
     "exception": false,
     "start_time": "2024-06-20T21:27:29.842994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dca30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:29.904828Z",
     "iopub.status.busy": "2024-06-20T21:27:29.904312Z",
     "iopub.status.idle": "2024-06-20T21:27:29.908161Z",
     "shell.execute_reply": "2024-06-20T21:27:29.907295Z"
    },
    "papermill": {
     "duration": 0.017308,
     "end_time": "2024-06-20T21:27:29.910006",
     "exception": false,
     "start_time": "2024-06-20T21:27:29.892698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def990a",
   "metadata": {
    "papermill": {
     "duration": 0.010338,
     "end_time": "2024-06-20T21:27:29.930802",
     "exception": false,
     "start_time": "2024-06-20T21:27:29.920464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the following cell there are all the other imports for running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fbb06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:29.952830Z",
     "iopub.status.busy": "2024-06-20T21:27:29.952537Z",
     "iopub.status.idle": "2024-06-20T21:27:48.655310Z",
     "shell.execute_reply": "2024-06-20T21:27:48.654351Z"
    },
    "papermill": {
     "duration": 18.716333,
     "end_time": "2024-06-20T21:27:48.657670",
     "exception": false,
     "start_time": "2024-06-20T21:27:29.941337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected accelerate version: 0.34.2\n",
      "Detected bitsandbytes version: 0.44.1\n",
      "Detected datasets version: 3.0.1\n",
      "Detected jinja2 version: 2.11.3\n",
      "Detected nltk version: 3.7\n",
      "Detected pandas version: 1.4.2\n",
      "Detected peft version: 0.13.0\n",
      "Detected psutil version: 5.8.0\n",
      "Detected pytest version: 7.1.1\n",
      "Detected safetensors version: 0.4.5\n",
      "Detected scipy version: 1.7.3\n",
      "Detected tokenizers version: 0.19.1\n",
      "Detected torchvision version: 0.15.2+cu117\n",
      "Detected torch version: 2.1.2+cu118\n",
      "Detected PIL version 9.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_default_log_level: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected rich version: 13.9.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import libraries: 3.96 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Bắt đầu tính thời gian\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Kết thúc tính thời gian\n",
    "end_time = time.time()\n",
    "print(f\"Time to import libraries: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9790589f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:48.681518Z",
     "iopub.status.busy": "2024-06-20T21:27:48.680853Z",
     "iopub.status.idle": "2024-06-20T21:27:48.685728Z",
     "shell.execute_reply": "2024-06-20T21:27:48.684866Z"
    },
    "papermill": {
     "duration": 0.018746,
     "end_time": "2024-06-20T21:27:48.687953",
     "exception": false,
     "start_time": "2024-06-20T21:27:48.669207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version 2.1.2+cu118\n",
      "working on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d07b47",
   "metadata": {
    "papermill": {
     "duration": 0.010421,
     "end_time": "2024-06-20T21:27:48.738326",
     "exception": false,
     "start_time": "2024-06-20T21:27:48.727905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Disabling two features in PyTorch related to memory efficiency and speed during operations on the Graphics Processing Unit (GPU) specifically for the scaled dot product attention (SDPA) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206a96eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:48.760963Z",
     "iopub.status.busy": "2024-06-20T21:27:48.760697Z",
     "iopub.status.idle": "2024-06-20T21:27:48.764508Z",
     "shell.execute_reply": "2024-06-20T21:27:48.763751Z"
    },
    "papermill": {
     "duration": 0.017241,
     "end_time": "2024-06-20T21:27:48.766411",
     "exception": false,
     "start_time": "2024-06-20T21:27:48.749170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a43dfa",
   "metadata": {
    "papermill": {
     "duration": 0.010576,
     "end_time": "2024-06-20T21:27:48.787897",
     "exception": false,
     "start_time": "2024-06-20T21:27:48.777321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing the data and the core evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceca451c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:48.831979Z",
     "iopub.status.busy": "2024-06-20T21:27:48.831740Z",
     "iopub.status.idle": "2024-06-20T21:27:50.245342Z",
     "shell.execute_reply": "2024-06-20T21:27:50.244625Z"
    },
    "papermill": {
     "duration": 1.427479,
     "end_time": "2024-06-20T21:27:50.247666",
     "exception": false,
     "start_time": "2024-06-20T21:27:48.820187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read data: 0.01 seconds\n",
      "Số lượng nhãn trong tập huấn luyện:\n",
      "neutral     1728\n",
      "positive     818\n",
      "negative     363\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Số lượng nhãn trong tập đánh giá:\n",
      "neutral     575\n",
      "positive    272\n",
      "negative    120\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Số lượng nhãn trong tập kiểm tra:\n",
      "neutral     576\n",
      "positive    273\n",
      "negative    121\n",
      "Name: sentiment, dtype: int64\n",
      "Time to create prompts: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "filename = \"all-data.csv\"\n",
    "\n",
    "# Bắt đầu tính thời gian cho việc đọc dữ liệu\n",
    "start_time_read = time.time()\n",
    "\n",
    "df = pd.read_csv(filename, \n",
    "                 names=[\"sentiment\", \"text\"],\n",
    "                 encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "end_time_read = time.time()\n",
    "print(f\"Time to read data: {end_time_read - start_time_read:.2f} seconds\")\n",
    "\n",
    "# X_train = list()\n",
    "# X_test = list()\n",
    "\n",
    "# # Bắt đầu tính thời gian cho việc chia tập dữ liệu\n",
    "# start_time_split = time.time()\n",
    "\n",
    "# for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "#     train, test  = train_test_split(df[df.sentiment == sentiment], \n",
    "#                                     train_size=300,\n",
    "#                                     test_size=300, \n",
    "#                                     random_state=42)\n",
    "#     X_train.append(train)\n",
    "#     X_test.append(test)\n",
    "\n",
    "# X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "# X_test = pd.concat(X_test)\n",
    "\n",
    "# # Kết thúc thời gian chia tập dữ liệu\n",
    "# end_time_split = time.time()\n",
    "# print(f\"Time to split data: {end_time_split - start_time_split:.2f} seconds\")\n",
    "\n",
    "# eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n",
    "# X_eval = df[df.index.isin(eval_idx)]\n",
    "# X_eval = (X_eval\n",
    "#           .groupby('sentiment', group_keys=False)\n",
    "#           .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "X_train = list()\n",
    "X_test = list()\n",
    "X_eval = list()\n",
    "\n",
    "# Bắt đầu tính thời gian cho việc chia tập dữ liệu\n",
    "start_time_split = time.time()\n",
    "\n",
    "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    # Chia dữ liệu thành tập huấn luyện và tập còn lại (test + eval)\n",
    "    train_val, test = train_test_split(df[df.sentiment == sentiment], \n",
    "                                       train_size=0.8,  # 80% cho train và eval\n",
    "                                       random_state=42)\n",
    "    \n",
    "    # Chia tập còn lại thành test và eval\n",
    "    eval_size = int(len(train_val) * 0.25)  # 20% của 80% là 20% tổng thể\n",
    "    train, eval = train_test_split(train_val, \n",
    "                                   test_size=eval_size, \n",
    "                                   random_state=42)\n",
    "    \n",
    "    # Thêm vào các danh sách\n",
    "    X_train.append(train)\n",
    "    X_eval.append(eval)\n",
    "    X_test.append(test)\n",
    "\n",
    "# Kết hợp các tập lại\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "X_eval = pd.concat(X_eval).reset_index(drop=True)\n",
    "X_test = pd.concat(X_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Thống kê số lượng nhãn cho từng tập\n",
    "train_label_counts = X_train['sentiment'].value_counts()\n",
    "eval_label_counts = X_eval['sentiment'].value_counts()\n",
    "test_label_counts = X_test['sentiment'].value_counts()\n",
    "\n",
    "# In ra kết quả thống kê\n",
    "print(\"Số lượng nhãn trong tập huấn luyện:\")\n",
    "print(train_label_counts)\n",
    "print(\"\\nSố lượng nhãn trong tập đánh giá:\")\n",
    "print(eval_label_counts)\n",
    "print(\"\\nSố lượng nhãn trong tập kiểm tra:\")\n",
    "print(test_label_counts)\n",
    "\n",
    "# Bắt đầu tính thời gian cho việc tạo prompt\n",
    "start_time_prompt = time.time()\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = \"\"\".strip()\n",
    "\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "# Kết thúc thời gian tạo prompt\n",
    "end_time_prompt = time.time()\n",
    "print(f\"Time to create prompts: {end_time_prompt - start_time_prompt:.2f} seconds\")\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e789580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Values ---\n",
      "Total records in df: 4846\n",
      "Total records in X_train: 2909\n",
      "Total records in X_eval: 967\n",
      "Total records in X_test: 970\n",
      "Total records in y_true: 970\n",
      "\n",
      "y_true distribution:\n",
      "neutral     576\n",
      "positive    273\n",
      "negative    121\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Train dataset:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2909\n",
      "})\n",
      "\n",
      "Eval dataset:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 967\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In ra giá trị của từng biến và tổng số lượng bản ghi\n",
    "print(\"\\n--- Final Values ---\")\n",
    "print(f\"Total records in df: {df.shape[0]}\")\n",
    "print(f\"Total records in X_train: {X_train.shape[0]}\")\n",
    "print(f\"Total records in X_eval: {X_eval.shape[0]}\")\n",
    "print(f\"Total records in X_test: {X_test.shape[0]}\")\n",
    "print(f\"Total records in y_true: {len(y_true)}\")\n",
    "print(\"\\ny_true distribution:\")\n",
    "print(y_true.value_counts())  # Hiển thị số lượng từng nhãn trong y_true\n",
    "print(\"\\nTrain dataset:\")\n",
    "print(train_data)\n",
    "print(\"\\nEval dataset:\")\n",
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dca745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Values ---\n",
      "X_train:\n",
      "                                                text\n",
      "0  Analyze the sentiment of the news headline enc...\n",
      "1  Analyze the sentiment of the news headline enc...\n",
      "2  Analyze the sentiment of the news headline enc...\n",
      "3  Analyze the sentiment of the news headline enc...\n",
      "4  Analyze the sentiment of the news headline enc...\n",
      "\n",
      "X_eval:\n",
      "                                                text\n",
      "0  Analyze the sentiment of the news headline enc...\n",
      "1  Analyze the sentiment of the news headline enc...\n",
      "2  Analyze the sentiment of the news headline enc...\n",
      "3  Analyze the sentiment of the news headline enc...\n",
      "4  Analyze the sentiment of the news headline enc...\n",
      "\n",
      "X_test:\n",
      "                                                text\n",
      "0  Analyze the sentiment of the news headline enc...\n",
      "1  Analyze the sentiment of the news headline enc...\n",
      "2  Analyze the sentiment of the news headline enc...\n",
      "3  Analyze the sentiment of the news headline enc...\n",
      "4  Analyze the sentiment of the news headline enc...\n",
      "\n",
      "y_true:\n",
      "neutral     576\n",
      "positive    273\n",
      "negative    121\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Train dataset:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2909\n",
      "})\n",
      "\n",
      "Eval dataset:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 967\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In ra giá trị của từng biến\n",
    "print(\"\\n--- Final Values ---\")\n",
    "print(\"X_train:\")\n",
    "print(X_train.head())  # Hiển thị 5 hàng đầu tiên của X_train\n",
    "print(\"\\nX_eval:\")\n",
    "print(X_eval.head())   # Hiển thị 5 hàng đầu tiên của X_eval\n",
    "print(\"\\nX_test:\")\n",
    "print(X_test.head())   # Hiển thị 5 hàng đầu tiên của X_test\n",
    "print(\"\\ny_true:\")\n",
    "print(y_true.value_counts())  # Hiển thị số lượng từng nhãn trong y_true\n",
    "print(\"\\nTrain dataset:\")\n",
    "print(train_data)\n",
    "print(\"\\nEval dataset:\")\n",
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c561132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Analyze the sentiment of the news headline enclosed in square brackets, \\n            determine if it is positive, neutral, or negative, and return the answer as \\n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\\n\\n            [The contractor of the shopping center , China State Construction Engineering Corporation , has previously built e.g. airports , hotels and factories for large international customers in different parts of the world .] = neutral'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731cb86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:50.292980Z",
     "iopub.status.busy": "2024-06-20T21:27:50.292462Z",
     "iopub.status.idle": "2024-06-20T21:27:50.301244Z",
     "shell.execute_reply": "2024-06-20T21:27:50.300457Z"
    },
    "papermill": {
     "duration": 0.022201,
     "end_time": "2024-06-20T21:27:50.303097",
     "exception": false,
     "start_time": "2024-06-20T21:27:50.280896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe3f54",
   "metadata": {
    "papermill": {
     "duration": 0.010322,
     "end_time": "2024-06-20T21:27:50.324139",
     "exception": false,
     "start_time": "2024-06-20T21:27:50.313817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing the model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf7d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Khôi phục trạng thái cảnh báo về mặc định\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# Nếu bạn đã từng dùng filterwarnings, không cần thiết phải xóa nó,\n",
    "# nhưng nếu có, bạn có thể xóa hoặc bình luận dòng này.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "044b5096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:27:50.368804Z",
     "iopub.status.busy": "2024-06-20T21:27:50.368264Z",
     "iopub.status.idle": "2024-06-20T21:29:38.348692Z",
     "shell.execute_reply": "2024-06-20T21:29:38.347700Z"
    },
    "papermill": {
     "duration": 107.994237,
     "end_time": "2024-06-20T21:29:38.350874",
     "exception": false,
     "start_time": "2024-06-20T21:27:50.356637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\g\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\g\n",
      "C:\\Users\\huyinit\\AppData\\Local\\Temp\\ipykernel_9328\\4236184942.py:5: DeprecationWarning: invalid escape sequence \\g\n",
      "  model_name = \"E:\\gemma\\gemma-transformers-2b-it-v3\"\n",
      "loading configuration file E:\\gemma\\gemma-transformers-2b-it-v3\\config.json\n",
      "Model config GemmaConfig {\n",
      "  \"_name_or_path\": \"E:\\\\gemma\\\\gemma-transformers-2b-it-v3\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading weights file E:\\gemma\\gemma-transformers-2b-it-v3\\model.safetensors.index.json\n",
      "Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975c95b8b4084da58d9f8a654d29e555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
      "\n",
      "All the weights of GemmaForCausalLM were initialized from the model checkpoint at E:\\gemma\\gemma-transformers-2b-it-v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
      "loading configuration file E:\\gemma\\gemma-transformers-2b-it-v3\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5.47 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bắt đầu tính thời gian\n",
    "start_time = time.time()\n",
    "\n",
    "# Đoạn mã của bạn\n",
    "model_name = \"E:\\gemma\\gemma-transformers-2b-it-v3\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',  # Thay thế device bằng 'auto' nếu bạn chưa định nghĩa\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 512  # 2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Kết thúc tính thời gian\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e29dc454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : GemmaTokenizerFast(name_or_path='E:\\gemma\\gemma-transformers-2b-it-v3', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<eos>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Tokenizer pad_token_id: 1\n",
      "Tokenizer eos_token_id: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In kết quả của tokenizer\n",
    "print(f\"Tokenizer : {tokenizer}\")\n",
    "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"Tokenizer eos_token_id: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5496c9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:29:38.396757Z",
     "iopub.status.busy": "2024-06-20T21:29:38.396457Z",
     "iopub.status.idle": "2024-06-20T21:29:38.403243Z",
     "shell.execute_reply": "2024-06-20T21:29:38.402425Z"
    },
    "papermill": {
     "duration": 0.020786,
     "end_time": "2024-06-20T21:29:38.405103",
     "exception": false,
     "start_time": "2024-06-20T21:29:38.384317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 1, \n",
    "                        temperature = 0.0,\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f934ced",
   "metadata": {
    "papermill": {
     "duration": 0.010771,
     "end_time": "2024-06-20T21:29:38.427142",
     "exception": false,
     "start_time": "2024-06-20T21:29:38.416371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At this point, we are ready to test the Llama 3 8b-chat-hf model and see how it performs on our problem without any fine-tuning. This allows us to get insights on the model itself and establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7964cc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:29:38.450231Z",
     "iopub.status.busy": "2024-06-20T21:29:38.449971Z",
     "iopub.status.idle": "2024-06-20T21:35:06.128568Z",
     "shell.execute_reply": "2024-06-20T21:35:06.127466Z"
    },
    "papermill": {
     "duration": 327.692355,
     "end_time": "2024-06-20T21:35:06.130523",
     "exception": false,
     "start_time": "2024-06-20T21:29:38.438168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/970 [00:00<?, ?it/s]c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 970/970 [00:45<00:00, 21.45it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0096410",
   "metadata": {
    "papermill": {
     "duration": 0.078974,
     "end_time": "2024-06-20T21:35:06.289711",
     "exception": false,
     "start_time": "2024-06-20T21:35:06.210737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 8b-chat-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859d0552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:35:06.450250Z",
     "iopub.status.busy": "2024-06-20T21:35:06.449863Z",
     "iopub.status.idle": "2024-06-20T21:35:06.470678Z",
     "shell.execute_reply": "2024-06-20T21:35:06.469800Z"
    },
    "papermill": {
     "duration": 0.103318,
     "end_time": "2024-06-20T21:35:06.472467",
     "exception": false,
     "start_time": "2024-06-20T21:35:06.369149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.599\n",
      "Accuracy for label 0: 0.000\n",
      "Accuracy for label 1: 0.997\n",
      "Accuracy for label 2: 0.026\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       121\n",
      "           1       0.60      1.00      0.75       576\n",
      "           2       0.58      0.03      0.05       273\n",
      "\n",
      "    accuracy                           0.60       970\n",
      "   macro avg       0.39      0.34      0.27       970\n",
      "weighted avg       0.52      0.60      0.46       970\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 118   3]\n",
      " [  0 574   2]\n",
      " [  0 266   7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8ca9d",
   "metadata": {
    "papermill": {
     "duration": 0.079321,
     "end_time": "2024-06-20T21:35:06.631557",
     "exception": false,
     "start_time": "2024-06-20T21:35:06.552236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12675811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:35:06.953374Z",
     "iopub.status.busy": "2024-06-20T21:35:06.953042Z",
     "iopub.status.idle": "2024-06-20T21:35:06.959429Z",
     "shell.execute_reply": "2024-06-20T21:35:06.958650Z"
    },
    "papermill": {
     "duration": 0.089793,
     "end_time": "2024-06-20T21:35:06.961309",
     "exception": false,
     "start_time": "2024-06-20T21:35:06.871516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, \n",
    "                             recall_score, \n",
    "                             precision_score, \n",
    "                             f1_score)\n",
    "\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de943e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T21:35:07.123517Z",
     "iopub.status.busy": "2024-06-20T21:35:07.122876Z",
     "iopub.status.idle": "2024-06-20T21:35:10.088120Z",
     "shell.execute_reply": "2024-06-20T21:35:10.087419Z"
    },
    "papermill": {
     "duration": 3.04909,
     "end_time": "2024-06-20T21:35:10.090159",
     "exception": false,
     "start_time": "2024-06-20T21:35:07.041069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:285: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:323: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:329: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a5e46cdbb046ecabf45c59852658b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9389d30513ef4f8993d54dcf3a4247d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/967 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:398: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"trained_weigths_2b\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=5,                       # number of training epochs\n",
    "    per_device_train_batch_size=1,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    # report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    #evaluation_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    #load_best_model_at_end = True,\n",
    "    #eval_steps = 25,\n",
    "    metric_for_best_model = 'accuracy',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129678a",
   "metadata": {
    "papermill": {
     "duration": 0.07897,
     "end_time": "2024-06-20T21:35:10.249984",
     "exception": false,
     "start_time": "2024-06-20T21:35:10.171014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6e43df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Debug start _inner_training_loop :****\n",
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 2,909\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1,815\n",
      "  Number of trainable parameters = 78,446,592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start def train(self, *args, **kwargs):\n",
      "start super().train(*args, **kwargs)\n",
      "Train! inner:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7114303eb6974610a080f7a740521f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch : 0\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4683, 'grad_norm': 1.503372073173523, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.07}\n",
      "{'loss': 1.3198, 'grad_norm': 0.7482884526252747, 'learning_rate': 0.0001781818181818182, 'epoch': 0.14}\n",
      "{'loss': 1.1366, 'grad_norm': 0.6585472822189331, 'learning_rate': 0.00019994249442005377, 'epoch': 0.21}\n",
      "{'loss': 1.1074, 'grad_norm': 0.5488616824150085, 'learning_rate': 0.0001996917333733128, 'epoch': 0.28}\n",
      "{'loss': 1.0553, 'grad_norm': 0.5926984548568726, 'learning_rate': 0.00019924248101441957, 'epoch': 0.34}\n",
      "{'loss': 1.0441, 'grad_norm': 0.6097349524497986, 'learning_rate': 0.00019859563182767268, 'epoch': 0.41}\n",
      "{'loss': 1.0235, 'grad_norm': 0.7097632884979248, 'learning_rate': 0.00019775247372270612, 'epoch': 0.48}\n",
      "{'loss': 1.0769, 'grad_norm': 0.5686435699462891, 'learning_rate': 0.00019671468547019573, 'epoch': 0.55}\n",
      "{'loss': 1.0011, 'grad_norm': 0.5231035947799683, 'learning_rate': 0.00019548433335934124, 'epoch': 0.62}\n",
      "{'loss': 1.0176, 'grad_norm': 0.5926935076713562, 'learning_rate': 0.00019406386708377955, 'epoch': 0.69}\n",
      "{'loss': 0.9419, 'grad_norm': 0.48243409395217896, 'learning_rate': 0.00019245611486411995, 'epoch': 0.76}\n",
      "{'loss': 0.9889, 'grad_norm': 0.4667383134365082, 'learning_rate': 0.00019066427781681315, 'epoch': 0.83}\n",
      "{'loss': 0.9629, 'grad_norm': 0.5035628080368042, 'learning_rate': 0.00018869192358056543, 'epoch': 0.89}\n",
      "{'loss': 0.9695, 'grad_norm': 0.5662674903869629, 'learning_rate': 0.00018654297921298863, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9033, 'grad_norm': 0.6036748886108398, 'learning_rate': 0.00018422172337162867, 'epoch': 1.03}\n",
      "{'loss': 0.8182, 'grad_norm': 0.5833869576454163, 'learning_rate': 0.0001817327777949407, 'epoch': 1.1}\n",
      "{'loss': 0.8041, 'grad_norm': 0.48762691020965576, 'learning_rate': 0.0001790810981001728, 'epoch': 1.17}\n",
      "{'loss': 0.7749, 'grad_norm': 0.539311945438385, 'learning_rate': 0.00017627196391647982, 'epoch': 1.24}\n",
      "{'loss': 0.7637, 'grad_norm': 0.6613630056381226, 'learning_rate': 0.00017331096837291344, 'epoch': 1.31}\n",
      "{'loss': 0.7887, 'grad_norm': 0.4996355473995209, 'learning_rate': 0.00017020400696221737, 'epoch': 1.38}\n",
      "{'loss': 0.7907, 'grad_norm': 0.5366184115409851, 'learning_rate': 0.00016695726580260174, 'epoch': 1.44}\n",
      "{'loss': 0.8373, 'grad_norm': 0.5460715889930725, 'learning_rate': 0.00016357720932086688, 'epoch': 1.51}\n",
      "{'loss': 0.7792, 'grad_norm': 0.5863980054855347, 'learning_rate': 0.00016007056738140085, 'epoch': 1.58}\n",
      "{'loss': 0.8335, 'grad_norm': 0.5560630559921265, 'learning_rate': 0.00015644432188667695, 'epoch': 1.65}\n",
      "{'loss': 0.7818, 'grad_norm': 0.6319857835769653, 'learning_rate': 0.00015270569287593091, 'epoch': 1.72}\n",
      "{'loss': 0.7593, 'grad_norm': 0.4139215052127838, 'learning_rate': 0.00014886212414969553, 'epoch': 1.79}\n",
      "{'loss': 0.8196, 'grad_norm': 0.55591881275177, 'learning_rate': 0.00014492126844881494, 'epoch': 1.86}\n",
      "{'loss': 0.7819, 'grad_norm': 0.5091454386711121, 'learning_rate': 0.00014089097221744868, 'epoch': 1.93}\n",
      "{'loss': 0.825, 'grad_norm': 0.5326573252677917, 'learning_rate': 0.00013677925998040188, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5547, 'grad_norm': 0.7590330839157104, 'learning_rate': 0.00013259431836588843, 'epoch': 2.06}\n",
      "{'loss': 0.5281, 'grad_norm': 0.744849681854248, 'learning_rate': 0.00012834447980553768, 'epoch': 2.13}\n",
      "{'loss': 0.5111, 'grad_norm': 0.6166107058525085, 'learning_rate': 0.00012403820594409924, 'epoch': 2.2}\n",
      "{'loss': 0.5358, 'grad_norm': 0.6611078977584839, 'learning_rate': 0.00011968407079187794, 'epoch': 2.27}\n",
      "{'loss': 0.5545, 'grad_norm': 0.877822756767273, 'learning_rate': 0.00011529074365344301, 'epoch': 2.34}\n",
      "{'loss': 0.537, 'grad_norm': 0.6277784705162048, 'learning_rate': 0.00011086697186660186, 'epoch': 2.41}\n",
      "{'loss': 0.5495, 'grad_norm': 0.6212083101272583, 'learning_rate': 0.00010642156338600551, 'epoch': 2.48}\n",
      "{'loss': 0.5532, 'grad_norm': 0.5910392999649048, 'learning_rate': 0.00010196336924606283, 'epoch': 2.54}\n",
      "{'loss': 0.5251, 'grad_norm': 0.7858157157897949, 'learning_rate': 9.750126593808082e-05, 'epoch': 2.61}\n",
      "{'loss': 0.532, 'grad_norm': 0.6475206613540649, 'learning_rate': 9.304413773671881e-05, 'epoch': 2.68}\n",
      "{'loss': 0.5339, 'grad_norm': 0.7804959416389465, 'learning_rate': 8.860085901094595e-05, 'epoch': 2.75}\n",
      "{'loss': 0.5747, 'grad_norm': 0.6126028299331665, 'learning_rate': 8.418027655472085e-05, 'epoch': 2.82}\n",
      "{'loss': 0.5317, 'grad_norm': 0.7581067085266113, 'learning_rate': 7.979119197257505e-05, 'epoch': 2.89}\n",
      "{'loss': 0.5565, 'grad_norm': 0.8531461358070374, 'learning_rate': 7.544234415517058e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4562, 'grad_norm': 0.5965243577957153, 'learning_rate': 7.114239187972416e-05, 'epoch': 3.03}\n",
      "{'loss': 0.3124, 'grad_norm': 0.7318397164344788, 'learning_rate': 6.689989656994124e-05, 'epoch': 3.09}\n",
      "{'loss': 0.3206, 'grad_norm': 0.6394571661949158, 'learning_rate': 6.272330524978613e-05, 'epoch': 3.16}\n",
      "{'loss': 0.3135, 'grad_norm': 0.6350642442703247, 'learning_rate': 5.862093372502731e-05, 'epoch': 3.23}\n",
      "{'loss': 0.3236, 'grad_norm': 0.6508257985115051, 'learning_rate': 5.4600950026045326e-05, 'epoch': 3.3}\n",
      "{'loss': 0.3138, 'grad_norm': 0.7786319255828857, 'learning_rate': 5.067135814486892e-05, 'epoch': 3.37}\n",
      "{'loss': 0.3327, 'grad_norm': 0.7147948741912842, 'learning_rate': 4.683998209881943e-05, 'epoch': 3.44}\n",
      "{'loss': 0.3215, 'grad_norm': 0.9407151937484741, 'learning_rate': 4.3114450352494704e-05, 'epoch': 3.51}\n",
      "{'loss': 0.3102, 'grad_norm': 0.7623529434204102, 'learning_rate': 3.9502180629107756e-05, 'epoch': 3.58}\n",
      "{'loss': 0.3109, 'grad_norm': 0.7057169675827026, 'learning_rate': 3.601036514142273e-05, 'epoch': 3.64}\n",
      "{'loss': 0.3311, 'grad_norm': 0.9957202076911926, 'learning_rate': 3.2645956271693257e-05, 'epoch': 3.71}\n",
      "{'loss': 0.3157, 'grad_norm': 0.7893396019935608, 'learning_rate': 2.9415652729115972e-05, 'epoch': 3.78}\n",
      "{'loss': 0.3196, 'grad_norm': 0.9743484854698181, 'learning_rate': 2.6325886212359498e-05, 'epoch': 3.85}\n",
      "{'loss': 0.3232, 'grad_norm': 0.9529817700386047, 'learning_rate': 2.3382808603725492e-05, 'epoch': 3.92}\n",
      "{'loss': 0.3128, 'grad_norm': 0.8408089280128479, 'learning_rate': 2.0592279720437858e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2373, 'grad_norm': 0.5802236795425415, 'learning_rate': 1.795985564744864e-05, 'epoch': 4.06}\n",
      "{'loss': 0.2082, 'grad_norm': 0.5402771830558777, 'learning_rate': 1.5490777674990376e-05, 'epoch': 4.13}\n",
      "{'loss': 0.209, 'grad_norm': 0.6317805647850037, 'learning_rate': 1.3189961862900447e-05, 'epoch': 4.19}\n",
      "{'loss': 0.2252, 'grad_norm': 0.7848302721977234, 'learning_rate': 1.1061989252496053e-05, 'epoch': 4.26}\n",
      "{'loss': 0.2202, 'grad_norm': 0.723427951335907, 'learning_rate': 9.111096745487779e-06, 'epoch': 4.33}\n",
      "{'loss': 0.2071, 'grad_norm': 0.7235187888145447, 'learning_rate': 7.341168668092857e-06, 'epoch': 4.4}\n",
      "{'loss': 0.2122, 'grad_norm': 0.5966668725013733, 'learning_rate': 5.755729037144097e-06, 'epoch': 4.47}\n",
      "{'loss': 0.2094, 'grad_norm': 0.7597913146018982, 'learning_rate': 4.357934543593045e-06, 'epoch': 4.54}\n",
      "{'loss': 0.2194, 'grad_norm': 0.7408391833305359, 'learning_rate': 3.150568267377818e-06, 'epoch': 4.61}\n",
      "{'loss': 0.2168, 'grad_norm': 0.5701523423194885, 'learning_rate': 2.1360341361692517e-06, 'epoch': 4.68}\n",
      "{'loss': 0.2131, 'grad_norm': 0.7165319323539734, 'learning_rate': 1.316352139028787e-06, 'epoch': 4.74}\n",
      "{'loss': 0.2206, 'grad_norm': 0.935223400592804, 'learning_rate': 6.931543045073708e-07, 'epoch': 4.81}\n",
      "{'loss': 0.2166, 'grad_norm': 0.651765763759613, 'learning_rate': 2.676814511936576e-07, 'epoch': 4.88}\n",
      "{'loss': 0.2107, 'grad_norm': 0.6931887269020081, 'learning_rate': 4.078071718107701e-08, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2800.3443, 'train_samples_per_second': 5.194, 'train_steps_per_second': 0.648, 'train_loss': 0.6312580631455771, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1815, training_loss=0.6312580631455771, metrics={'train_runtime': 2800.3443, 'train_samples_per_second': 5.194, 'train_steps_per_second': 0.648, 'total_flos': 1.53297898484736e+16, 'train_loss': 0.6312580631455771, 'epoch': 4.99140598143692})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d46c",
   "metadata": {
    "papermill": {
     "duration": 0.080058,
     "end_time": "2024-06-20T23:12:53.967740",
     "exception": false,
     "start_time": "2024-06-20T23:12:53.887682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model and the tokenizer are saved to disk for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58941266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T23:12:54.129906Z",
     "iopub.status.busy": "2024-06-20T23:12:54.129530Z",
     "iopub.status.idle": "2024-06-20T23:12:55.802127Z",
     "shell.execute_reply": "2024-06-20T23:12:55.801137Z"
    },
    "papermill": {
     "duration": 1.756601,
     "end_time": "2024-06-20T23:12:55.804545",
     "exception": false,
     "start_time": "2024-06-20T23:12:54.047944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_weigths_2b\n",
      "loading configuration file E:\\gemma\\gemma-transformers-2b-it-v3\\config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in trained_weigths_2b\\tokenizer_config.json\n",
      "Special tokens file saved in trained_weigths_2b\\special_tokens_map.json\n",
      "tokenizer config file saved in trained_weigths_2b\\tokenizer_config.json\n",
      "Special tokens file saved in trained_weigths_2b\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('trained_weigths_2b\\\\tokenizer_config.json',\n",
       " 'trained_weigths_2b\\\\special_tokens_map.json',\n",
       " 'trained_weigths_2b\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57d92a",
   "metadata": {
    "papermill": {
     "duration": 0.079959,
     "end_time": "2024-06-20T23:12:55.965656",
     "exception": false,
     "start_time": "2024-06-20T23:12:55.885697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Afterwards, loading the TensorBoard extension and start TensorBoard, pointing to the logs/runs directory, which is assumed to contain the training logs and checkpoints for your model, will allow you to understand how the models fits during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb5884",
   "metadata": {
    "papermill": {
     "duration": 0.087667,
     "end_time": "2024-06-20T23:13:02.350704",
     "exception": false,
     "start_time": "2024-06-20T23:13:02.263037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2dafb5",
   "metadata": {
    "papermill": {
     "duration": 0.0822,
     "end_time": "2024-06-20T23:13:02.520927",
     "exception": false,
     "start_time": "2024-06-20T23:13:02.438727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's performance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0479fbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T23:13:02.698193Z",
     "iopub.status.busy": "2024-06-20T23:13:02.697380Z",
     "iopub.status.idle": "2024-06-20T23:19:07.764652Z",
     "shell.execute_reply": "2024-06-20T23:19:07.763596Z"
    },
    "papermill": {
     "duration": 365.151257,
     "end_time": "2024-06-20T23:19:07.766680",
     "exception": false,
     "start_time": "2024-06-20T23:13:02.615423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/970 [00:00<?, ?it/s]c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\huyinit\\anaconda3\\envs\\llama3_212\\lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 970/970 [01:02<00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n",
      "Accuracy for label 0: 0.876\n",
      "Accuracy for label 1: 0.899\n",
      "Accuracy for label 2: 0.799\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88       121\n",
      "           1       0.89      0.90      0.89       576\n",
      "           2       0.82      0.80      0.81       273\n",
      "\n",
      "    accuracy                           0.87       970\n",
      "   macro avg       0.86      0.86      0.86       970\n",
      "weighted avg       0.87      0.87      0.87       970\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[106  13   2]\n",
      " [ 11 518  47]\n",
      " [  2  53 218]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320f1ed",
   "metadata": {
    "papermill": {
     "duration": 0.148146,
     "end_time": "2024-06-20T23:19:08.064687",
     "exception": false,
     "start_time": "2024-06-20T23:19:07.916541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code will create a Pandas DataFrame called evaluation containing the text, true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and gettting insights on how to improve the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce1c5aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T23:19:08.364899Z",
     "iopub.status.busy": "2024-06-20T23:19:08.364197Z",
     "iopub.status.idle": "2024-06-20T23:19:08.388932Z",
     "shell.execute_reply": "2024-06-20T23:19:08.388237Z"
    },
    "papermill": {
     "duration": 0.176911,
     "end_time": "2024-06-20T23:19:08.390850",
     "exception": false,
     "start_time": "2024-06-20T23:19:08.213939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions_2b.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 622510,
     "sourceId": 1192499,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llama3_212",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6838.673758,
   "end_time": "2024-06-20T23:19:12.346812",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-20T21:25:13.673054",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00d8d3148d174c20b5e91b8147fdbcd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b899350b67349b2b940186b84946a53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_37c18815c6664d2eab7b5cb3a8c993bc",
        "IPY_MODEL_8496dd2d1c3642b0b31e63b3091c65ea",
        "IPY_MODEL_9d820edaebd24601bfa027c079c12155"
       ],
       "layout": "IPY_MODEL_00d8d3148d174c20b5e91b8147fdbcd4"
      }
     },
     "29015cb862314aa1b112c42770e1c3e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b875d46b3d344b9bee90938d70f331c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d5cc86756e2478fb361507e7f07a4e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2f5ba648f1ff49e3ab198f0852e72891": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_988c04b443e74e5bbe9222a44ba51775",
       "max": 900,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d811750bf1144d10b824b893b2ea260d",
       "value": 900
      }
     },
     "2f74bc32c08842a8b0b4009b52a79512": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "346509a3d92948aeb5c0ac773dfb42b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37c18815c6664d2eab7b5cb3a8c993bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_29015cb862314aa1b112c42770e1c3e2",
       "placeholder": "​",
       "style": "IPY_MODEL_a315751b5d13489dbef67d7006b104b7",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "57c5ffa894a74a8faaed08fc88d4d2ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5bbec511bbae4591995c7a78e6b0c80b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_346509a3d92948aeb5c0ac773dfb42b9",
       "placeholder": "​",
       "style": "IPY_MODEL_8c82fee8112a4c38a4d4b37a641a7831",
       "value": " 900/900 [00:00&lt;00:00, 4069.54 examples/s]"
      }
     },
     "5f064603cb9a4b5a9d674bf08c3824bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e0e10bafb640b68962698a4cbb5638",
       "placeholder": "​",
       "style": "IPY_MODEL_2d5cc86756e2478fb361507e7f07a4e3",
       "value": "Map: 100%"
      }
     },
     "7743078c7fc743ae9dd7d5217aefb511": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8496dd2d1c3642b0b31e63b3091c65ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7743078c7fc743ae9dd7d5217aefb511",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cbe68a0487f54bbdbdcaae752d2545e2",
       "value": 4
      }
     },
     "8b562ece5ce246859b435c3b0955f815": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5f064603cb9a4b5a9d674bf08c3824bb",
        "IPY_MODEL_2f5ba648f1ff49e3ab198f0852e72891",
        "IPY_MODEL_5bbec511bbae4591995c7a78e6b0c80b"
       ],
       "layout": "IPY_MODEL_2b875d46b3d344b9bee90938d70f331c"
      }
     },
     "8c82fee8112a4c38a4d4b37a641a7831": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "988c04b443e74e5bbe9222a44ba51775": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d820edaebd24601bfa027c079c12155": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f74bc32c08842a8b0b4009b52a79512",
       "placeholder": "​",
       "style": "IPY_MODEL_57c5ffa894a74a8faaed08fc88d4d2ec",
       "value": " 4/4 [01:46&lt;00:00, 22.84s/it]"
      }
     },
     "a315751b5d13489dbef67d7006b104b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e0e10bafb640b68962698a4cbb5638": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbe68a0487f54bbdbdcaae752d2545e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d811750bf1144d10b824b893b2ea260d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
